{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Aq405erfpGKv"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "raB0pq_vuaak",
    "outputId": "482c0c4c-96e0-4625-f4c7-285501aacc0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5Y4Y3eXugTc",
    "outputId": "9b85a79b-7268-4188-ee10-3c542457a52f"
   },
   "outputs": [],
   "source": [
    "env_id = \"CarRacing-v2\"\n",
    "\n",
    "# Create the env\n",
    "env = gym.make(env_id, continuous=False, domain_randomize=False)\n",
    "\n",
    "# Create the evaluation env\n",
    "eval_env = gym.make(env_id, continuous=False, domain_randomize=False)\n",
    "\n",
    "# Get the state space and action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "n_frames = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NpSZGAuKulI6"
   },
   "outputs": [],
   "source": [
    "from policy import Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5kAeq1Rl1Hyj"
   },
   "outputs": [],
   "source": [
    "MAX_PATIENCE = 100 # Maximum consecutive steps with negative reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sYk1se-R3vmh"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, n_eval_episodes, policy):\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_eval_episodes):\n",
    "        state = env.reset() # state reset\n",
    "        \n",
    "        # perform noop for 60 steps (noisy start)\n",
    "        for i in range(60):\n",
    "            state,_,_,_,_ = env.step(0)\n",
    "            \n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # stats\n",
    "        total_rewards_ep = 0\n",
    "        negative_reward_patience = MAX_PATIENCE\n",
    "        \n",
    "        # state\n",
    "        states = deque(maxlen=4)\n",
    "        for i in range(n_frames):\n",
    "            states.append(state)\n",
    "            \n",
    "        while not done:\n",
    "            # perform action\n",
    "            action, _ = policy.act(states, exploration=False)\n",
    "            \n",
    "            state, reward, done, info, _ = env.step(action)\n",
    "            states.append(state)\n",
    "            \n",
    "            # handle patience\n",
    "            if reward >=0:\n",
    "                negative_reward_patience = MAX_PATIENCE\n",
    "            else:\n",
    "                negative_reward_patience -= 1\n",
    "                if negative_reward_patience == 0:\n",
    "                    done = True\n",
    "            if done: reward = -100\n",
    "                    \n",
    "            # stats\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # stats\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "        \n",
    "    # stats\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Uzb4bInRxMsx"
   },
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes=1000, gamma=0.99, print_every=5):\n",
    "    # stats\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    \n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = [] # stores log probs during episode\n",
    "        rewards = [] # stores rewards during episode\n",
    "        \n",
    "        # init episode\n",
    "        state = env.reset()\n",
    "        for i in range(60):\n",
    "            state,_,_,_,_ = env.step(0)\n",
    "        done = False\n",
    "        \n",
    "        negative_reward_patience = MAX_PATIENCE\n",
    "        states = deque(maxlen=4)\n",
    "        for i in range(n_frames):\n",
    "            states.append(state)\n",
    "            \n",
    "            \n",
    "        while not done:\n",
    "            action, log_prob = policy.act(states)\n",
    "            \n",
    "            # store log_prob\n",
    "            ...\n",
    "            \n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            states.append(state)\n",
    "            \n",
    "            if reward >=0:\n",
    "                negative_reward_patience = MAX_PATIENCE\n",
    "            else:\n",
    "                negative_reward_patience -= 1\n",
    "                if negative_reward_patience == 0:\n",
    "                    done = True\n",
    "            if done: reward = -100\n",
    "                    \n",
    "            # store reward\n",
    "            ...\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "\n",
    "        \n",
    "        rewards = np.array(rewards)\n",
    "        discounts = np.power(gamma, np.arange(len(rewards)))\n",
    "        \n",
    "        policy_loss = 0\n",
    "        for t in range(len(rewards)):\n",
    "            G = ... # Return from timestep t\n",
    "            policy_loss += ... # loss for timestep t\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print(f'''Episode {i_episode}\n",
    "                    \\tAverage Score: {np.mean(scores_deque)}\n",
    "                    \\tLast Score: {rewards.sum()}\n",
    "                    \\tEval Score: {evaluate_agent(eval_env,5,policy)}''')\n",
    "            torch.save(policy, 'model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(n_frames, n_actions, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bQYqOdbiy0ez"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esS1pRU6D9CS",
    "outputId": "21b11365-29ed-45d3-add8-568ed969baaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5\n",
      "                    \tAverage Score: -126.85806083002448\n",
      "                    \tLast Score: -127.06923076923087\n",
      "                    \tEval Score: (-21.808294065704075, 44.87709334907354)\n",
      "Episode 10\n",
      "                    \tAverage Score: -135.28766947803575\n",
      "                    \tLast Score: -189.54364261168215\n",
      "                    \tEval Score: (-54.311251396611055, 4.2408577769040035)\n",
      "Episode 15\n",
      "                    \tAverage Score: -137.35474418697868\n",
      "                    \tLast Score: -153.9933993399349\n",
      "                    \tEval Score: (-37.90619181510813, 37.25444640135546)\n",
      "Episode 20\n",
      "                    \tAverage Score: -134.32653074863458\n",
      "                    \tLast Score: -136.98015267175634\n",
      "                    \tEval Score: (-27.67147149595551, 41.0123308702339)\n",
      "Episode 25\n",
      "                    \tAverage Score: -133.596227172179\n",
      "                    \tLast Score: -117.80818505338063\n",
      "                    \tEval Score: (-19.542297466909268, 42.53915552477403)\n",
      "Episode 30\n",
      "                    \tAverage Score: -129.168860128847\n",
      "                    \tLast Score: -106.7399267399266\n",
      "                    \tEval Score: (-44.976599563803596, 25.998968153135923)\n",
      "Episode 35\n",
      "                    \tAverage Score: -124.88412425479447\n",
      "                    \tLast Score: -98.98333333333312\n",
      "                    \tEval Score: (-57.315483650118026, 2.900497262322308)\n",
      "Episode 40\n",
      "                    \tAverage Score: -123.732652434478\n",
      "                    \tLast Score: -109.47483443708606\n",
      "                    \tEval Score: (-52.71862034807443, 2.1511691663868726)\n"
     ]
    }
   ],
   "source": [
    "reinforce(policy, optimizer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
