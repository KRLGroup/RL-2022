{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3595f2b1",
   "metadata": {},
   "source": [
    "# Definition of a 3-layer neural net with tanh activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d89c548",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13208\\1240542875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
    "        super().__init__()\n",
    "        self.activation_function= nn.Tanh()\n",
    "\n",
    "        self.layer1 = nn.Linear( #<--- linear layer\n",
    "            n_inputs, #<----------------#input features\n",
    "            64,#<-----------------------#output features\n",
    "            bias=bias)#<----------------bias\n",
    "\n",
    "        self.layer2 = nn.Linear(\n",
    "            64,\n",
    "            32,\n",
    "            bias=bias)\n",
    "\n",
    "        self.layer3 = nn.Linear(\n",
    "                    32,\n",
    "                    n_outputs,\n",
    "                    bias=bias)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation_function( self.layer1(x) )\n",
    "        x = self.activation_function( self.layer2(x) )\n",
    "        y = self.layer3(x)\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15988814",
   "metadata": {},
   "source": [
    "# Q network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb414e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_network(nn.Module):\n",
    "\n",
    "    def __init__(self, env,  learning_rate=1e-4):\n",
    "        super(Q_network, self).__init__()\n",
    "\n",
    "        #TODO\n",
    "        #self.network = Net( ?? , ??)\n",
    "        self.network = Net( 1,1)\n",
    "        \n",
    "        print(\"Q network:\")\n",
    "        print(self.network)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(),\n",
    "                                          lr=learning_rate)\n",
    "\n",
    "    def greedy_action(self, state):\n",
    "        # TODO\n",
    "        # greedy action = ??\n",
    "        greedy_a = 0\n",
    "\n",
    "        return greedy_a\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        #TODO\n",
    "        #qval = ?\n",
    "        qval = 0\n",
    "        return qval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4333d140",
   "metadata": {},
   "source": [
    "## Experience replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "\n",
    "class Experience_replay_buffer:\n",
    "\n",
    "    def __init__(self, memory_size=50000, burn_in=10000):\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.Buffer = namedtuple('Buffer',\n",
    "                                 field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
    "        self.replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        samples = np.random.choice(len(self.replay_memory), batch_size,\n",
    "                                   replace=False)\n",
    "        # Use asterisk operator to unpack deque\n",
    "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
    "        return batch\n",
    "\n",
    "    def append(self, s_0, a, r, d, s_1):\n",
    "        self.replay_memory.append(\n",
    "            self.Buffer(s_0, a, r, d, s_1))\n",
    "\n",
    "    def burn_in_capacity(self):\n",
    "        return len(self.replay_memory) / self.burn_in\n",
    "\n",
    "    def capacity(self):\n",
    "        return len(self.replay_memory) / self.memory_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a12d08",
   "metadata": {},
   "source": [
    "# DDQN agent implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f2db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import from_tuple_to_tensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#with warnings.catch_warnings():\n",
    "#    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "def from_tuple_to_tensor(tuple_of_np):\n",
    "    tensor = torch.zeros((len(tuple_of_np), tuple_of_np[0].shape[0]))\n",
    "    for i, x in enumerate(tuple_of_np):\n",
    "        tensor[i] = torch.FloatTensor(x)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "class DDQN_agent:\n",
    "\n",
    "    def __init__(self, env, rew_thre, buffer, learning_rate=0.001, initial_epsilon=0.5, batch_size= 64):\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "\n",
    "        self.network = Q_network(env, learning_rate)\n",
    "        #TODO\n",
    "        #self.target_network = ???\n",
    "        self.buffer = buffer\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.window = 50\n",
    "        self.reward_threshold = rew_thre\n",
    "        self.initialize()\n",
    "        self.step_count = 0\n",
    "        self.episode = 0\n",
    "\n",
    "\n",
    "    def take_step(self, mode='exploit'):\n",
    "        # choose action with epsilon greedy\n",
    "        #TODO\n",
    "        #action = ??\n",
    "\n",
    "\n",
    "        #simulate action\n",
    "        s_1, r, done, _, _ = self.env.step(action)\n",
    "\n",
    "\n",
    "        #put experience in the buffer\n",
    "        #TODO\n",
    "        # self.buffer ??\n",
    "\n",
    "        self.rewards += r\n",
    "\n",
    "        self.s_0 = s_1.copy()\n",
    "\n",
    "        self.step_count += 1\n",
    "        if done:\n",
    "\n",
    "            self.s_0, _ = self.env.reset()\n",
    "        return done\n",
    "\n",
    "    # Implement DQN training algorithm\n",
    "    def train(self, gamma=0.99, max_episodes=10000,\n",
    "              network_update_frequency=10,\n",
    "              network_sync_frequency=100):\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.s_0, _ = self.env.reset()\n",
    "\n",
    "        # Populate replay buffer\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(mode='explore')\n",
    "        ep = 0\n",
    "        training = True\n",
    "        self.populate = False\n",
    "        while training:\n",
    "            self.s_0, _ = self.env.reset()\n",
    "\n",
    "            self.rewards = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                if ((ep % 5) == 0):\n",
    "                    self.env.render()\n",
    "\n",
    "                p = np.random.random()\n",
    "                if p < self.epsilon:\n",
    "                    done = self.take_step(mode='explore')\n",
    "                else:\n",
    "                    done = self.take_step(mode='exploit')\n",
    "                # Update network\n",
    "                if self.step_count % network_update_frequency == 0:\n",
    "                    self.update()\n",
    "                # Sync networks\n",
    "                if self.step_count % network_sync_frequency == 0:\n",
    "                    # TODO: synchronize Qnet and target_net\n",
    "                    #self.target_network ???\n",
    "                    self.sync_eps.append(ep)\n",
    "\n",
    "                if done:\n",
    "                    if self.epsilon >= 0.05:\n",
    "                        self.epsilon = self.epsilon * 0.7\n",
    "                    ep += 1\n",
    "                    self.training_rewards.append(self.rewards)\n",
    "                    self.training_loss.append(np.mean(self.update_loss))\n",
    "                    self.update_loss = []\n",
    "                    mean_rewards = np.mean(\n",
    "                        self.training_rewards[-self.window:])\n",
    "                    mean_loss = np.mean(self.training_loss[-self.window:])\n",
    "                    self.mean_training_rewards.append(mean_rewards)\n",
    "                    print(\n",
    "                        \"\\rEpisode {:d} Mean Rewards {:.2f}  Episode reward = {:.2f}   mean loss = {:.2f} \\t\\t\".format(\n",
    "                            ep, mean_rewards, self.rewards, mean_loss), end=\"\")\n",
    "\n",
    "                    if ep >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "                    if mean_rewards >= self.reward_threshold:\n",
    "                        training = False\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            ep))\n",
    "                        #break\n",
    "        # save models\n",
    "        self.save_models()\n",
    "        # plot\n",
    "        self.plot_training_rewards()\n",
    "\n",
    "    def save_models(self):\n",
    "        torch.save(self.network, \"Q_net\")\n",
    "\n",
    "    def load_models(self):\n",
    "        self.network = torch.load(\"Q_net\")\n",
    "        self.network.eval()\n",
    "\n",
    "    def plot_training_rewards(self):\n",
    "        plt.plot(self.mean_training_rewards)\n",
    "        plt.title('Mean training rewards')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.xlabel('Episods')\n",
    "        plt.show()\n",
    "        plt.savefig('mean_training_rewards.png')\n",
    "        plt.clf()\n",
    "\n",
    "    def calculate_loss(self, batch):\n",
    "        #extract info from batch\n",
    "        states, actions, rewards, dones, next_states = list(batch)\n",
    "\n",
    "        #transform in torch tensors\n",
    "        rewards = torch.FloatTensor(rewards).reshape(-1, 1).to(device)\n",
    "        actions = torch.LongTensor(np.array(actions)).reshape(-1, 1).to(device)\n",
    "        dones = torch.IntTensor(dones).reshape(-1, 1).to(device)\n",
    "        states = from_tuple_to_tensor(states)\n",
    "        next_states = from_tuple_to_tensor(next_states)\n",
    "\n",
    "        ###############\n",
    "        # DDQN Update #\n",
    "        ###############\n",
    "        #TODO\n",
    "        # Q(s,a) = ??\n",
    "        #\n",
    "        #\n",
    "\n",
    "        # TODO\n",
    "        # target Q(s,a) = ??\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "\n",
    "\n",
    "        #TODO\n",
    "        #loss = self.loss_function( Q(s,a) , target_Q(s,a))\n",
    "        loss = 0\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        #zero tha gradients\n",
    "        self.network.optimizer.zero_grad()\n",
    "        \n",
    "        #sample data from experience replay buffer\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size)\n",
    "        #calculate loss\n",
    "        loss = self.calculate_loss(batch)\n",
    "        #compute the gradients\n",
    "        loss.backward()\n",
    "        #update the weigths\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "        if device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n",
    "\n",
    "    def initialize(self):\n",
    "        self.training_rewards = []\n",
    "        self.training_loss = []\n",
    "        self.update_loss = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.sync_eps = []\n",
    "        self.rewards = 0\n",
    "        self.step_count = 0\n",
    "\n",
    "    def evaluate(self, eval_env):\n",
    "        done = False\n",
    "        s, _ = eval_env.reset()\n",
    "        rew = 0\n",
    "        while not done:\n",
    "            action = self.network.greedy_action(torch.FloatTensor(s))\n",
    "            s, r, done, _, _ =eval_env.step(action)\n",
    "            rew += r\n",
    "\n",
    "        print(\"Evaluation cumulative reward: \", rew)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c20dcd",
   "metadata": {},
   "source": [
    "# Train and evaluate on cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e742a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "rew_threshold = 400\n",
    "buffer = Experience_replay_buffer()\n",
    "agent = DDQN_agent(env, rew_threshold, buffer)\n",
    "agent.train()\n",
    "\n",
    "eval_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "agent.evaluate(eval_env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
