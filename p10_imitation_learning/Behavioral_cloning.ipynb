{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b09a0b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elena/PycharmProjects/esercitazioni_RL/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from tqdm import tqdm\n",
    "#from .autonotebook import tqdm as notebook_tqdm\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d945059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define env\n",
    "env_id = \"CartPole-v1\"\n",
    "#env_id = \"Acrobot-v1\"\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3daca912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'Acrobot-v1'\n",
      "Creating environment from the given name 'Acrobot-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/user/1000/app/com.jetbrains.PyCharm-Community/ipykernel_1479/3493697535.py:2: DeprecationWarning: The parameter `create_eval_env` is deprecated and will be removed in the future. Please use `EvalCallback` or a custom Callback instead.\n",
      "  ppo_expert = PPO('MlpPolicy', env_id, verbose=1, create_eval_env=True)\n",
      "/run/user/1000/app/com.jetbrains.PyCharm-Community/ipykernel_1479/3493697535.py:5: DeprecationWarning: Parameters `eval_env` and `eval_freq` are deprecated and will be removed in the future. Please use `EvalCallback` or a custom Callback instead.\n",
      "  ppo_expert.learn(total_timesteps=3e4, eval_freq=10000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | -500     |\n",
      "| time/              |          |\n",
      "|    fps             | 1697     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | -500        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1261        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009083457 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.0217     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.4        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00609    |\n",
      "|    value_loss           | 132         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 495        |\n",
      "|    ep_rew_mean          | -495       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1165       |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00871703 |\n",
      "|    clip_fraction        | 0.049      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | 0.000705   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 21.8       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00409   |\n",
      "|    value_loss           | 111        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 478          |\n",
      "|    ep_rew_mean          | -477         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1120         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068241917 |\n",
      "|    clip_fraction        | 0.0411       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | -0.428       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.2         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00707     |\n",
      "|    value_loss           | 98           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-175.00 +/- 162.76\n",
      "Episode length: 175.80 +/- 162.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 176         |\n",
      "|    mean_reward          | -175        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008390799 |\n",
      "|    clip_fraction        | 0.0648      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21          |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00647    |\n",
      "|    value_loss           | 82          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 404      |\n",
      "|    ep_rew_mean     | -404     |\n",
      "| time/              |          |\n",
      "|    fps             | 1042     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 357         |\n",
      "|    ep_rew_mean          | -356        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1033        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009941414 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.0467      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.5        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 82.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 316          |\n",
      "|    ep_rew_mean          | -316         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1027         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074197487 |\n",
      "|    clip_fraction        | 0.0746       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.998       |\n",
      "|    explained_variance   | 0.149        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.6         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0056      |\n",
      "|    value_loss           | 83.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 290         |\n",
      "|    ep_rew_mean          | -289        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1023        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009280162 |\n",
      "|    clip_fraction        | 0.0761      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.983      |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.2        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00857    |\n",
      "|    value_loss           | 76.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 269          |\n",
      "|    ep_rew_mean          | -268         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1017         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076344097 |\n",
      "|    clip_fraction        | 0.0914       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.935       |\n",
      "|    explained_variance   | 0.61         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.7         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00961     |\n",
      "|    value_loss           | 60           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-340.80 +/- 195.84\n",
      "Episode length: 341.20 +/- 195.35\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 341        |\n",
      "|    mean_reward          | -341       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00877653 |\n",
      "|    clip_fraction        | 0.062      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.877     |\n",
      "|    explained_variance   | 0.751      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 11.4       |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.00624   |\n",
      "|    value_loss           | 48.5       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 249      |\n",
      "|    ep_rew_mean     | -248     |\n",
      "| time/              |          |\n",
      "|    fps             | 973      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 230          |\n",
      "|    ep_rew_mean          | -229         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 974          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085445065 |\n",
      "|    clip_fraction        | 0.0771       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.849       |\n",
      "|    explained_variance   | 0.75         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.9         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00868     |\n",
      "|    value_loss           | 46.8         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 172        |\n",
      "|    ep_rew_mean          | -171       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 977        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 25         |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00855102 |\n",
      "|    clip_fraction        | 0.0831     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.774     |\n",
      "|    explained_variance   | 0.845      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 24.2       |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.00677   |\n",
      "|    value_loss           | 40.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 156         |\n",
      "|    ep_rew_mean          | -155        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 980         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005669014 |\n",
      "|    clip_fraction        | 0.0521      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.711      |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.9        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00565    |\n",
      "|    value_loss           | 37.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 137          |\n",
      "|    ep_rew_mean          | -136         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 981          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049223364 |\n",
      "|    clip_fraction        | 0.0303       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.631       |\n",
      "|    explained_variance   | 0.818        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.1         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00409     |\n",
      "|    value_loss           | 33.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-84.40 +/- 4.67\n",
      "Episode length: 85.40 +/- 4.67\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 85.4         |\n",
      "|    mean_reward          | -84.4        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057665147 |\n",
      "|    clip_fraction        | 0.0608       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.617       |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.1         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00509     |\n",
      "|    value_loss           | 26.3         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 125      |\n",
      "|    ep_rew_mean     | -124     |\n",
      "| time/              |          |\n",
      "|    fps             | 974      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elena/PycharmProjects/esercitazioni_RL/venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward expert agent= -121.4 +/- 85.71254283942345\n"
     ]
    }
   ],
   "source": [
    "#define expert agent\n",
    "ppo_expert = PPO('MlpPolicy', env_id, verbose=1, create_eval_env=True)\n",
    "\n",
    "#train expert\n",
    "ppo_expert.learn(total_timesteps=3e4, eval_freq=10000)\n",
    "\n",
    "#save expert\n",
    "ppo_expert.save(\"ppo_expert\")\n",
    "\n",
    "#evaluate expert\n",
    "mean_reward, std_reward = evaluate_policy(ppo_expert, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward expert agent= {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "480991d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 6)\n",
      "(40000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:17<00:00, 2227.12it/s]\n"
     ]
    }
   ],
   "source": [
    "##create expert dataset\n",
    "\n",
    "num_interactions = int(4e4)\n",
    "\n",
    "#define empty dataset\n",
    "expert_observations = ...np...\n",
    "expert_actions = ...np...\n",
    "\n",
    "\n",
    "#collect experience usign expert policy\n",
    "obs = env.reset()\n",
    "for i in tqdm(range(num_interactions)):\n",
    "    action, _ = ppo_expert.predict(obs, deterministic=True)\n",
    "    \n",
    "    #TODO: do a step\n",
    "    \n",
    "    expert_observations[i] = ...\n",
    "    expert_actions[i] = ...\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a75579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset\n",
    "np.savez_compressed(\n",
    "   \"expert_data\",\n",
    "   expert_actions=expert_actions,\n",
    "   expert_observations=expert_observations,\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce03945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##dataset class\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "\n",
    "class ExpertDataSet(Dataset):\n",
    "\n",
    "    def __init__(self, expert_observations, expert_actions):\n",
    "        self.observations = expert_observations\n",
    "        self.actions = expert_actions\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.observations[index], self.actions[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11af5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_dataset = ExpertDataSet(expert_observations, expert_actions)\n",
    "\n",
    "#TODO: split in 80% training and 20%test\n",
    "batch_size = 64\n",
    "train_prop = 0.8\n",
    "\n",
    "train_expert_dataset = ...\n",
    "test_expert_dataset = ...\n",
    "\n",
    "train_loader = th.utils.data.DataLoader(  dataset=train_expert_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = th.utils.data.DataLoader(  dataset=test_expert_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c7a5b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Define student agent\n",
    "no_cuda = False\n",
    "use_cuda = not no_cuda and th.cuda.is_available()\n",
    "   \n",
    "device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "class StudentAgent:\n",
    "    def __init__(self, env, train_loader, test_loader, learning_rate):\n",
    "        self.env = env\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        #TODO: define a policy network\n",
    "        self.policy = ...th...\n",
    "        \n",
    "        print(\"policy net: \", self.policy)\n",
    "        \n",
    "        #TODO: define loss criterion\n",
    "        self.loss_criterion = ...\n",
    "        \n",
    "        #TODO: define optimizer\n",
    "        self.optimizer =  ...\n",
    "        \n",
    "        self.num_eval_episodes = 10\n",
    "        \n",
    "    def train(self, num_epochs):\n",
    "        self.policy.train()\n",
    "        self.policy.to(device)\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                obs, expert_action = data.to(device), target.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                obs = obs.float()\n",
    "                \n",
    "                #TODO\n",
    "                student_action = ...\n",
    "                loss = ...\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            #compute accuracy\n",
    "            train_acc = self.compute_accuracy(self.train_loader)\n",
    "            test_acc = self.compute_accuracy(self.test_loader)\n",
    "            policy_return = self.evaluate_policy(self.num_eval_episodes)\n",
    "            print(\"Epoch {}:\\ttrain accuracy: {}\\ttest accuracy: {}\\tpolicy return:{}\".format(epoch, train_acc, test_acc, policy_return))\n",
    "\n",
    "    def compute_accuracy(self, loader):\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        \n",
    "        self.policy.eval()\n",
    "        test_loss = 0\n",
    "        with th.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                obs, expert_action = data.to(device), target.to(device)\n",
    "                obs = obs.float()\n",
    "                \n",
    "                #TODO\n",
    "                student_action = ...\n",
    "            \n",
    "                total += ...\n",
    "                correct += ...\n",
    "            \n",
    "        accuracy = 100. * correct/(float)(total)\n",
    "            \n",
    "        return accuracy\n",
    "            \n",
    "    def policy_action(self, obs):\n",
    "        #TODO\n",
    "        policy_act = ...\n",
    "        \n",
    "        return policy_act\n",
    "        \n",
    "    def evaluate_policy(self, num_episodes, render=False):\n",
    "        rewards = []\n",
    "        for ep in range(num_episodes):\n",
    "            done = False\n",
    "            tot_rew = 0\n",
    "            obs = self.env.reset()\n",
    "\n",
    "            while not done:\n",
    "                #TODO\n",
    "                action = ...\n",
    "                \n",
    "                obs, reward, done, info = env.step(action)\n",
    "                if render:\n",
    "                    env.render()\n",
    "                tot_rew += reward\n",
    "            rewards.append(tot_rew)\n",
    "        return mean(rewards)\n",
    "    \n",
    "\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "614e3ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy net:  Sequential(\n",
      "  (0): Linear(in_features=6, out_features=16, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=16, out_features=3, bias=True)\n",
      "  (3): Softmax(dim=-1)\n",
      ")\n",
      "Epoch 0:\ttrain accuracy: 99.675\ttest accuracy: 99.675\tpolicy return:-86.5\n",
      "Epoch 1:\ttrain accuracy: 99.6125\ttest accuracy: 99.6125\tpolicy return:-79.3\n",
      "Epoch 2:\ttrain accuracy: 99.675\ttest accuracy: 99.675\tpolicy return:-84.5\n",
      "Epoch 3:\ttrain accuracy: 99.5375\ttest accuracy: 99.5375\tpolicy return:-76.7\n",
      "Epoch 4:\ttrain accuracy: 99.675\ttest accuracy: 99.675\tpolicy return:-78.8\n",
      "Epoch 5:\ttrain accuracy: 99.7625\ttest accuracy: 99.7625\tpolicy return:-79.3\n",
      "Epoch 6:\ttrain accuracy: 99.775\ttest accuracy: 99.775\tpolicy return:-81.8\n",
      "Epoch 7:\ttrain accuracy: 99.4\ttest accuracy: 99.4\tpolicy return:-88.9\n",
      "Epoch 8:\ttrain accuracy: 99.6875\ttest accuracy: 99.6875\tpolicy return:-86.6\n",
      "Epoch 9:\ttrain accuracy: 99.6875\ttest accuracy: 99.6875\tpolicy return:-82.3\n",
      "Epoch 10:\ttrain accuracy: 99.7625\ttest accuracy: 99.7625\tpolicy return:-80.9\n",
      "Epoch 11:\ttrain accuracy: 99.625\ttest accuracy: 99.625\tpolicy return:-78.7\n",
      "Epoch 12:\ttrain accuracy: 99.6125\ttest accuracy: 99.6125\tpolicy return:-74.6\n",
      "Epoch 13:\ttrain accuracy: 99.7375\ttest accuracy: 99.7375\tpolicy return:-92.8\n",
      "Epoch 14:\ttrain accuracy: 99.6\ttest accuracy: 99.6\tpolicy return:-83.0\n",
      "Epoch 15:\ttrain accuracy: 99.75\ttest accuracy: 99.75\tpolicy return:-83.3\n",
      "Epoch 16:\ttrain accuracy: 99.675\ttest accuracy: 99.675\tpolicy return:-82.4\n",
      "Epoch 17:\ttrain accuracy: 99.525\ttest accuracy: 99.525\tpolicy return:-78.7\n",
      "Epoch 18:\ttrain accuracy: 99.7125\ttest accuracy: 99.7125\tpolicy return:-77.0\n",
      "Epoch 19:\ttrain accuracy: 99.6875\ttest accuracy: 99.6875\tpolicy return:-80.3\n",
      "Epoch 20:\ttrain accuracy: 99.725\ttest accuracy: 99.725\tpolicy return:-84.3\n",
      "Epoch 21:\ttrain accuracy: 99.475\ttest accuracy: 99.475\tpolicy return:-88.5\n",
      "Epoch 22:\ttrain accuracy: 99.8125\ttest accuracy: 99.8125\tpolicy return:-79.8\n",
      "Epoch 23:\ttrain accuracy: 99.775\ttest accuracy: 99.775\tpolicy return:-86.5\n",
      "Epoch 24:\ttrain accuracy: 99.6625\ttest accuracy: 99.6625\tpolicy return:-81.6\n",
      "Epoch 25:\ttrain accuracy: 99.7375\ttest accuracy: 99.7375\tpolicy return:-97.5\n",
      "Epoch 26:\ttrain accuracy: 99.5125\ttest accuracy: 99.5125\tpolicy return:-79.6\n",
      "Epoch 27:\ttrain accuracy: 99.6875\ttest accuracy: 99.6875\tpolicy return:-88.0\n",
      "Epoch 28:\ttrain accuracy: 99.6875\ttest accuracy: 99.6875\tpolicy return:-79.3\n",
      "Epoch 29:\ttrain accuracy: 99.7875\ttest accuracy: 99.7875\tpolicy return:-79.7\n",
      "Epoch 30:\ttrain accuracy: 99.7125\ttest accuracy: 99.7125\tpolicy return:-79.2\n",
      "Epoch 31:\ttrain accuracy: 99.725\ttest accuracy: 99.725\tpolicy return:-86.1\n",
      "Epoch 32:\ttrain accuracy: 99.8\ttest accuracy: 99.8\tpolicy return:-81.5\n",
      "Epoch 33:\ttrain accuracy: 99.5875\ttest accuracy: 99.5875\tpolicy return:-80.4\n",
      "Epoch 34:\ttrain accuracy: 99.675\ttest accuracy: 99.675\tpolicy return:-93.4\n",
      "Epoch 35:\ttrain accuracy: 99.825\ttest accuracy: 99.825\tpolicy return:-78.7\n",
      "Epoch 36:\ttrain accuracy: 99.7\ttest accuracy: 99.7\tpolicy return:-82.4\n",
      "Epoch 37:\ttrain accuracy: 99.7625\ttest accuracy: 99.7625\tpolicy return:-80.8\n",
      "Epoch 38:\ttrain accuracy: 99.775\ttest accuracy: 99.775\tpolicy return:-87.5\n",
      "Epoch 39:\ttrain accuracy: 99.8\ttest accuracy: 99.8\tpolicy return:-80.2\n",
      "Epoch 40:\ttrain accuracy: 99.8\ttest accuracy: 99.8\tpolicy return:-80.9\n",
      "Epoch 41:\ttrain accuracy: 99.7375\ttest accuracy: 99.7375\tpolicy return:-80.1\n",
      "Epoch 42:\ttrain accuracy: 99.8\ttest accuracy: 99.8\tpolicy return:-89.7\n",
      "Epoch 43:\ttrain accuracy: 99.7875\ttest accuracy: 99.7875\tpolicy return:-88.9\n",
      "Epoch 44:\ttrain accuracy: 99.6625\ttest accuracy: 99.6625\tpolicy return:-81.6\n",
      "Epoch 45:\ttrain accuracy: 99.6125\ttest accuracy: 99.6125\tpolicy return:-79.9\n",
      "Epoch 46:\ttrain accuracy: 99.6875\ttest accuracy: 99.6875\tpolicy return:-90.1\n",
      "Epoch 47:\ttrain accuracy: 99.725\ttest accuracy: 99.725\tpolicy return:-93.4\n",
      "Epoch 48:\ttrain accuracy: 99.7625\ttest accuracy: 99.7625\tpolicy return:-82.0\n",
      "Epoch 49:\ttrain accuracy: 99.675\ttest accuracy: 99.675\tpolicy return:-82.0\n"
     ]
    }
   ],
   "source": [
    "student = StudentAgent(env, train_loader, test_loader, 0.01)\n",
    "student.train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462442b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72e7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
