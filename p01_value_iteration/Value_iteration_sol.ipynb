{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73118f47",
   "metadata": {},
   "source": [
    "# Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec0869",
   "metadata": {},
   "source": [
    "![](imgs/value_iteration_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e70165",
   "metadata": {},
   "source": [
    "![](imgs/value_iteration_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5451f934",
   "metadata": {},
   "source": [
    "![](imgs/value_iteration_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52736fc",
   "metadata": {},
   "source": [
    "![](imgs/value_iteration_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09972ce1",
   "metadata": {},
   "source": [
    "![](imgs/value_iteration_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfc96a0",
   "metadata": {},
   "source": [
    "![](imgs/value_iteration_6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2523810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from gym import spaces\n",
    "import os\n",
    "\n",
    "\n",
    "# custom 2d grid world enviroment\n",
    "class GridWorld(gym.Env):\n",
    "    metadata = {'render.modes': ['console']}\n",
    "\n",
    "    # actions available\n",
    "    UP = 0\n",
    "    LEFT = 1\n",
    "    DOWN = 2\n",
    "    RIGHT = 3\n",
    "\n",
    "\n",
    "    def __init__(self, width, height):\n",
    "        super(GridWorld, self).__init__()\n",
    "        self.ACTION_NAMES = [\"UP\", \"LEFT\", \"DOWN\", \"RIGHT\"]\n",
    "        self.num_actions = 4\n",
    "\n",
    "        self.size = width * height  # size of the grid world\n",
    "        self.num_states = self.size\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_obstacles = int((width+height)/2)\n",
    "        self.end_state = np.array([height - 1, width - 1], dtype=np.uint8) # goal state = bottom right cell\n",
    "\n",
    "        # actions of agents : up, down, left and right\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # observation : cell indices in the grid\n",
    "        self.observation_space = spaces.MultiDiscrete([self.height, self.width])\n",
    "\n",
    "        self.obstacles = np.zeros((height, width))\n",
    "\n",
    "        for i in range(self.num_obstacles):\n",
    "            self.obstacles[ random.randrange(height) , random.randrange(width)] = 1\n",
    "\n",
    "        self.num_steps = 0\n",
    "        self.max_steps = height*width\n",
    "\n",
    "        self.current_state = np.zeros((2), np.uint8)#init state = [0,0]\n",
    "\n",
    "        self.directions = np.array([\n",
    "            [-1,0], #UP\n",
    "            [0,-1], #LEFT\n",
    "            [1,0], #DOWN\n",
    "            [0,1] #RIGHT\n",
    "        ])\n",
    "\n",
    "    def transition_function(self, s, a):\n",
    "        s_prime =  np.zeros((2), np.uint8)\n",
    "        #s_prime = ?????\n",
    "        s_prime = s + self.directions[a,:]\n",
    "\n",
    "        if s_prime[0] < self.height and s_prime[1] < self.width and (s_prime >= 0).all():\n",
    "            if self.obstacles[s_prime[0], s_prime[1]] == 0 :\n",
    "                return s_prime\n",
    "\n",
    "        return s\n",
    "\n",
    "    def transition_probabilities(self, s, a):\n",
    "        prob_next_state = np.zeros((self.heigth, self.width))\n",
    "        s_prime = self.transition_function(s, a)\n",
    "\n",
    "        prob_next_state[s_prime[0], s_prime[1]] = 1.0\n",
    "\n",
    "        return prob_next_state#.flatten()\n",
    "\n",
    "    def reward_function(self,s):\n",
    "        r = 0\n",
    "        #r= ????\n",
    "        if (s == self.end_state).all():\n",
    "            r = 1\n",
    "\n",
    "        return r\n",
    "\n",
    "    def termination_condition(self, s):\n",
    "        done = False\n",
    "        #done= ???\n",
    "\n",
    "        done = (s == self.end_state).all() or self.num_steps > self.max_steps\n",
    "\n",
    "        return done\n",
    "\n",
    "    def step(self, action):\n",
    "        s_prime = self.transition_function(self.current_state, action)\n",
    "        reward = self.reward_function(s_prime)\n",
    "        done = self.termination_condition(s_prime)\n",
    "\n",
    "        self.current_state = s_prime\n",
    "        self.num_steps += 1\n",
    "\n",
    "        return self.current_state, reward, done, None\n",
    "\n",
    "    def render(self):\n",
    "        '''\n",
    "            render the state\n",
    "        '''\n",
    "\n",
    "        row = self.current_state[0]\n",
    "        col = self.current_state[1]\n",
    "\n",
    "        for r in range(self.height):\n",
    "            for c in range(self.width):\n",
    "                if r == row and c == col:\n",
    "                    print(\"| A \", end='')\n",
    "                elif r == self.end_state[0] and c == self.end_state[1]:\n",
    "                    print(\"| G \", end='')\n",
    "                else:\n",
    "                    if self.obstacles[r,c] == 1:\n",
    "                        print('|///', end='')\n",
    "                    else:\n",
    "                        print('|___', end='')\n",
    "            print('|')\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = np.zeros((2), np.uint8)\n",
    "        self.num_steps = 0\n",
    "        return self.current_state\n",
    "\n",
    "    def reward_probabilities(self):\n",
    "        rewards = np.zeros((self.num_states))\n",
    "        i = 0\n",
    "        for r in range(self.height):\n",
    "            for c in range(self.width):\n",
    "                state = np.array([r,c], dtype=np.uint8)\n",
    "                rewards[i] = self.reward_function(state)\n",
    "                i+=1\n",
    "\n",
    "        return rewards\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class NonDeterministicGridWorld(GridWorld):\n",
    "    def __init__(self, width, height, p=0.8):\n",
    "        super(NonDeterministicGridWorld, self).__init__(width, height)\n",
    "        self.probability_right_action = p\n",
    "\n",
    "    def transition_function(self, s, a):\n",
    "        s_prime = s + self.directions[a, :]\n",
    "\n",
    "        #with probability 1 - p diagonal movement\n",
    "        if random.random() <= 1 - self.probability_right_action:\n",
    "            if random.random() < 0.5:\n",
    "                s_prime = s_prime + self.directions[(a+1)%self.num_actions, :]\n",
    "            else:\n",
    "                s_prime = s_prime + self.directions[(a-1)%self.num_actions, :]\n",
    "\n",
    "\n",
    "        if s_prime[0] < self.height and s_prime[1] < self.width and (s_prime >= 0).all():\n",
    "            if self.obstacles[s_prime[0], s_prime[1]] == 0 :\n",
    "                return s_prime\n",
    "\n",
    "        return s\n",
    "\n",
    "    def transition_probabilities(self, s, a):\n",
    "        cells = []\n",
    "        probs = []\n",
    "        prob_next_state = np.zeros((self.height, self.width))\n",
    "        s_prime_right =  s + self.directions[a, :]\n",
    "        if s_prime_right[0] < self.height and s_prime_right[1] < self.width and (s_prime_right >= 0).all():\n",
    "            if self.obstacles[s_prime_right[0], s_prime_right[1]] == 0 :\n",
    "                prob_next_state[s_prime_right[0], s_prime_right[1]] = self.probability_right_action\n",
    "                cells.append(s_prime_right)\n",
    "                probs.append(self.probability_right_action)\n",
    "\n",
    "        s_prime = s_prime_right + self.directions[(a + 1) % self.num_actions, :]\n",
    "        if s_prime[0] < self.height and s_prime[1] < self.width and (s_prime >= 0).all():\n",
    "            if self.obstacles[s_prime[0], s_prime[1]] == 0 :\n",
    "                prob_next_state[s_prime[0], s_prime[1]] = (1 - self.probability_right_action) / 2\n",
    "                cells.append(s_prime.copy())\n",
    "                probs.append((1 - self.probability_right_action) / 2)\n",
    "\n",
    "        s_prime = s_prime_right + self.directions[(a - 1) % self.num_actions, :]\n",
    "        if s_prime[0] < self.height and s_prime[1] < self.width and (s_prime >= 0).all():\n",
    "            if self.obstacles[s_prime[0], s_prime[1]] == 0 :\n",
    "                prob_next_state[s_prime[0], s_prime[1]] = (1 - self.probability_right_action) / 2\n",
    "                cells.append(s_prime.copy())\n",
    "                probs.append((1 - self.probability_right_action) / 2)\n",
    "\n",
    "        #normalization\n",
    "        sump = sum(probs)\n",
    "        #for cell in cells:\n",
    "        #    prob_next_state[cell[0], cell[1]] /= sump\n",
    "        prob_next_state[s[0], s[1]] = 1 - sump\n",
    "        return prob_next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602977ce",
   "metadata": {},
   "source": [
    "To apply value iteration we need the **transition probabilities** and the **reward function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dddba2",
   "metadata": {},
   "source": [
    "Print the probability over the next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "378bbbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| A |___|___|\n",
      "|///|///|___|\n",
      "|___|///|___|\n",
      "|___|///|___|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env = NonDeterministicGridWorld(3,5)\n",
    "state = env.reset()\n",
    "env.render()\n",
    "#next state if we start from state 0,0 and we do action down\n",
    "next_state_prob = env.transition_probabilities(state, 2)\n",
    "print(next_state_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61c23d",
   "metadata": {},
   "source": [
    "reward values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d9826985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(env.reward_probabilities())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71dea30",
   "metadata": {},
   "source": [
    "# Value iteration algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5512d",
   "metadata": {},
   "source": [
    "![](imgs/value_iteration.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "408efa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.99):\n",
    "\n",
    "    #initialize values\n",
    "    values = np.zeros((env.num_states))\n",
    "    STATES = np.zeros((env.num_states, 2), dtype=np.uint8)\n",
    "    REWARDS = env.reward_probabilities()\n",
    "    i = 0\n",
    "    for r in range(env.height):\n",
    "        for c in range(env.width):\n",
    "            state = np.array([r, c], dtype=np.uint8)\n",
    "            STATES[i] = state\n",
    "            i += 1\n",
    "    delta = 1\n",
    "    i = 0\n",
    "    while delta > 0.1:\n",
    "        v_old = values.copy()\n",
    "        for s in range(env.num_states):\n",
    "            if s == env.num_states-1 or i >= env.max_steps:\n",
    "                continue # if we reach the termination condition, we cannot perform any action\n",
    "            state = STATES[s]\n",
    "            max_va = -np.inf\n",
    "            for a in range(env.num_actions):\n",
    "                next_state_prob = env.transition_probabilities(state, a).flatten()\n",
    "                va = (next_state_prob*(REWARDS + gamma*v_old))\n",
    "                if s == env.num_states-1:\n",
    "                    print(va, va.sum())\n",
    "                \n",
    "                va = va.sum()\n",
    "\n",
    "                if va > max_va:\n",
    "                    max_va = va\n",
    "            values[s] = max_va\n",
    "        print(values.reshape((env.height, env.width)))\n",
    "        print()\n",
    "        delta = abs(sum(v_old - values))\n",
    "        i+=1\n",
    "        #print(delta)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dcbb0d",
   "metadata": {},
   "source": [
    "estimate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1dfa419c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.  0. ]\n",
      " [0.  0.  0. ]\n",
      " [0.  0.  0. ]\n",
      " [0.  0.1 0.8]\n",
      " [0.  0.8 0. ]]\n",
      "\n",
      "[[0.     0.     0.    ]\n",
      " [0.     0.     0.    ]\n",
      " [0.     0.0792 0.6336]\n",
      " [0.0792 0.7336 0.9584]\n",
      " [0.6336 0.9584 0.    ]]\n",
      "\n",
      "[[0.        0.        0.       ]\n",
      " [0.        0.0627264 0.5018112]\n",
      " [0.0627264 0.5966928 0.8845056]\n",
      " [0.6045336 0.9217792 0.9897632]\n",
      " [0.8845056 0.9897632 0.       ]]\n",
      "\n",
      "[[0.         0.04967931 0.39743447]\n",
      " [0.04967931 0.48500052 0.79988705]\n",
      " [0.49121044 0.8481943  0.95902456]\n",
      " [0.85836382 0.97145851 0.99597311]\n",
      " [0.95902456 0.99597311 0.        ]]\n",
      "\n",
      "[[0.03934601 0.39887517 0.71220257]\n",
      " [0.39887517 0.76779999 0.91792509]\n",
      " [0.77708381 0.93733761 0.97869757]\n",
      " [0.94312681 0.98375414 0.99720268]\n",
      " [0.97869757 0.99720268 0.        ]]\n",
      "\n",
      "[[0.32369965 0.69442766 0.86801278]\n",
      " [0.69442766 0.89439579 0.95687764]\n",
      " [0.90081903 0.96472612 0.98356664]\n",
      " [0.96722109 0.98667558 0.99744613]\n",
      " [0.98356664 0.99744613 0.        ]]\n",
      "\n",
      "[[0.61407924 0.85094535 0.92971362]\n",
      " [0.85094535 0.94115346 0.96844655]\n",
      " [0.94440127 0.97246283 0.98472353]\n",
      " [0.97348683 0.98735043 0.99749433]\n",
      " [0.98472353 0.99749433 0.        ]]\n",
      "\n",
      "[[0.7955364  0.91645299 0.95109297]\n",
      " [0.91645299 0.95653895 0.97165345]\n",
      " [0.95799302 0.97452918 0.98499077]\n",
      " [0.97502817 0.98750314 0.99750388]\n",
      " [0.98499077 0.99750388 0.        ]]\n",
      "\n",
      "[[0.88334697 0.94018817 0.95786594]\n",
      " [0.94018817 0.96122182 0.97250007]\n",
      " [0.96190493 0.97505927 0.98505124]\n",
      " [0.97539336 0.98753716 0.99750577]\n",
      " [0.98505124 0.99750577 0.        ]]\n",
      "\n",
      "[[0.91953173 0.94798596 0.95987752]\n",
      " [0.94798596 0.96256886 0.9727156 ]\n",
      " [0.96296872 0.97519116 0.98506471]\n",
      " [0.9754776  0.98754464 0.99750614]\n",
      " [0.98506471 0.99750614 0.        ]]\n",
      "\n",
      "[[0.91953173 0.94798596 0.95987752]\n",
      " [0.94798596 0.96256886 0.9727156 ]\n",
      " [0.96296872 0.97519116 0.98506471]\n",
      " [0.9754776  0.98754464 0.99750614]\n",
      " [0.98506471 0.99750614 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "values = value_iteration(env)\n",
    "values[-1] = 1\n",
    "values_r = values.reshape((env.height, env.width))\n",
    "print(values_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09159f81",
   "metadata": {},
   "source": [
    "Estimate best action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f6a427b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_action(env, values, s):\n",
    "    max_va = 0\n",
    "    best_a = -1\n",
    "    for a in range(env.num_actions):\n",
    "        expected_value_a = (env.transition_probabilities(s,a).flatten() * values).sum()\n",
    "        \n",
    "        ###### Expected value a\n",
    "        \n",
    "        #expected_value_a = ???\n",
    "        \n",
    "        ########\n",
    "        print(env.ACTION_NAMES[a], expected_value_a)\n",
    "        if expected_value_a > max_va:\n",
    "            max_va = expected_value_a\n",
    "            best_a = a\n",
    "    \n",
    "    return best_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68ae40",
   "metadata": {},
   "source": [
    "simulate optimal policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "edfe0c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UP 0.9195317289207765\n",
      "LEFT 0.9195317289207765\n",
      "DOWN 0.9195317289207765\n",
      "RIGHT 0.9422951150062722\n",
      "|___| A |___|\n",
      "|///|///|___|\n",
      "|___|///|___|\n",
      "|___|///|___|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "UP 0.9479859615276461\n",
      "LEFT 0.9252225754421505\n",
      "DOWN 0.9504589253802889\n",
      "RIGHT 0.9599721683672656\n",
      "|___|___| A |\n",
      "|///|///|___|\n",
      "|___|///|___|\n",
      "|___|///|___|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "UP 0.9598775152613671\n",
      "LEFT 0.9503642722743904\n",
      "DOWN 0.970147983095533\n",
      "RIGHT 0.9598775152613671\n",
      "|___|___|___|\n",
      "|///|///| A |\n",
      "|___|///|___|\n",
      "|___|///|___|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "UP 0.9599721683672656\n",
      "LEFT 0.9702426362014316\n",
      "DOWN 0.9825948915953288\n",
      "RIGHT 0.9727156000540744\n",
      "|___|___|___|\n",
      "|///|///|___|\n",
      "|___|///| A |\n",
      "|___|///|___|\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "UP 0.9751854229393879\n",
      "LEFT 0.9850647144806424\n",
      "DOWN 0.9950178565250765\n",
      "RIGHT 0.9850647144806424\n",
      "|___|___|___|\n",
      "|///|///|___|\n",
      "|___|///|___|\n",
      "|___|///| A |\n",
      "|___|___| G |\n",
      "\n",
      "\n",
      "UP 0.9875529999917508\n",
      "LEFT 0.997506142036185\n",
      "DOWN 0.999501228407237\n",
      "RIGHT 0.997506142036185\n",
      "|___|___|___|\n",
      "|///|///|___|\n",
      "|___|///|___|\n",
      "|___|///|___|\n",
      "|___|___| A |\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state = env.reset()\n",
    "while not done:\n",
    "    action = best_action(env, values,state)\n",
    "    \n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509fa835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c337ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
